{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,Conv1D,MaxPooling2D,Activation,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"rdt_new3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1n</th>\n",
       "      <th>2n</th>\n",
       "      <th>3n</th>\n",
       "      <th>4n</th>\n",
       "      <th>5n</th>\n",
       "      <th>6r</th>\n",
       "      <th>7r</th>\n",
       "      <th>8r</th>\n",
       "      <th>9r</th>\n",
       "      <th>10r</th>\n",
       "      <th>...</th>\n",
       "      <th>92r</th>\n",
       "      <th>93r</th>\n",
       "      <th>94r</th>\n",
       "      <th>95r</th>\n",
       "      <th>96r</th>\n",
       "      <th>97r</th>\n",
       "      <th>98r</th>\n",
       "      <th>99r</th>\n",
       "      <th>100r</th>\n",
       "      <th>101r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022902</td>\n",
       "      <td>0.662567</td>\n",
       "      <td>0.034351</td>\n",
       "      <td>0.148139</td>\n",
       "      <td>0.493378</td>\n",
       "      <td>0.671918</td>\n",
       "      <td>0.615551</td>\n",
       "      <td>0.625976</td>\n",
       "      <td>0.647064</td>\n",
       "      <td>0.578182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619049</td>\n",
       "      <td>0.748539</td>\n",
       "      <td>0.588579</td>\n",
       "      <td>0.679175</td>\n",
       "      <td>0.744493</td>\n",
       "      <td>0.587385</td>\n",
       "      <td>0.571649</td>\n",
       "      <td>0.732362</td>\n",
       "      <td>0.743735</td>\n",
       "      <td>0.530578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024732</td>\n",
       "      <td>0.658107</td>\n",
       "      <td>0.033129</td>\n",
       "      <td>0.146617</td>\n",
       "      <td>0.478424</td>\n",
       "      <td>0.667990</td>\n",
       "      <td>0.610094</td>\n",
       "      <td>0.620036</td>\n",
       "      <td>0.641139</td>\n",
       "      <td>0.574126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603915</td>\n",
       "      <td>0.752731</td>\n",
       "      <td>0.587314</td>\n",
       "      <td>0.685208</td>\n",
       "      <td>0.741377</td>\n",
       "      <td>0.585352</td>\n",
       "      <td>0.564472</td>\n",
       "      <td>0.732922</td>\n",
       "      <td>0.743326</td>\n",
       "      <td>0.532095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.653591</td>\n",
       "      <td>0.031421</td>\n",
       "      <td>0.148883</td>\n",
       "      <td>0.486036</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.621272</td>\n",
       "      <td>0.623157</td>\n",
       "      <td>0.634809</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616818</td>\n",
       "      <td>0.754496</td>\n",
       "      <td>0.590404</td>\n",
       "      <td>0.676380</td>\n",
       "      <td>0.738477</td>\n",
       "      <td>0.582982</td>\n",
       "      <td>0.568050</td>\n",
       "      <td>0.733060</td>\n",
       "      <td>0.743631</td>\n",
       "      <td>0.527555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.649135</td>\n",
       "      <td>0.030034</td>\n",
       "      <td>0.146710</td>\n",
       "      <td>0.451634</td>\n",
       "      <td>0.666542</td>\n",
       "      <td>0.638415</td>\n",
       "      <td>0.608425</td>\n",
       "      <td>0.667469</td>\n",
       "      <td>0.572128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600729</td>\n",
       "      <td>0.732823</td>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>0.735578</td>\n",
       "      <td>0.575488</td>\n",
       "      <td>0.575268</td>\n",
       "      <td>0.732713</td>\n",
       "      <td>0.742071</td>\n",
       "      <td>0.541149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030741</td>\n",
       "      <td>0.640175</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.464734</td>\n",
       "      <td>0.664265</td>\n",
       "      <td>0.632699</td>\n",
       "      <td>0.612745</td>\n",
       "      <td>0.652520</td>\n",
       "      <td>0.567949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583046</td>\n",
       "      <td>0.741828</td>\n",
       "      <td>0.588947</td>\n",
       "      <td>0.667850</td>\n",
       "      <td>0.730141</td>\n",
       "      <td>0.577973</td>\n",
       "      <td>0.579028</td>\n",
       "      <td>0.729647</td>\n",
       "      <td>0.740905</td>\n",
       "      <td>0.534735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>0.081394</td>\n",
       "      <td>0.446675</td>\n",
       "      <td>0.091244</td>\n",
       "      <td>0.298324</td>\n",
       "      <td>0.763869</td>\n",
       "      <td>0.582584</td>\n",
       "      <td>0.637223</td>\n",
       "      <td>0.610751</td>\n",
       "      <td>0.585631</td>\n",
       "      <td>0.609958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697665</td>\n",
       "      <td>0.694346</td>\n",
       "      <td>0.705105</td>\n",
       "      <td>0.740672</td>\n",
       "      <td>0.596873</td>\n",
       "      <td>0.551078</td>\n",
       "      <td>0.727091</td>\n",
       "      <td>0.739228</td>\n",
       "      <td>0.512227</td>\n",
       "      <td>0.458557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.089386</td>\n",
       "      <td>0.458004</td>\n",
       "      <td>0.096382</td>\n",
       "      <td>0.299258</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.577321</td>\n",
       "      <td>0.635280</td>\n",
       "      <td>0.600362</td>\n",
       "      <td>0.587635</td>\n",
       "      <td>0.615288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715464</td>\n",
       "      <td>0.712684</td>\n",
       "      <td>0.704778</td>\n",
       "      <td>0.732992</td>\n",
       "      <td>0.595095</td>\n",
       "      <td>0.545270</td>\n",
       "      <td>0.725116</td>\n",
       "      <td>0.736531</td>\n",
       "      <td>0.509141</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>0.078236</td>\n",
       "      <td>0.429350</td>\n",
       "      <td>0.099725</td>\n",
       "      <td>0.304498</td>\n",
       "      <td>0.745902</td>\n",
       "      <td>0.593526</td>\n",
       "      <td>0.630973</td>\n",
       "      <td>0.628439</td>\n",
       "      <td>0.581926</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748459</td>\n",
       "      <td>0.751056</td>\n",
       "      <td>0.692863</td>\n",
       "      <td>0.731864</td>\n",
       "      <td>0.593254</td>\n",
       "      <td>0.557480</td>\n",
       "      <td>0.728852</td>\n",
       "      <td>0.742128</td>\n",
       "      <td>0.518212</td>\n",
       "      <td>0.439950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.071266</td>\n",
       "      <td>0.421421</td>\n",
       "      <td>0.102860</td>\n",
       "      <td>0.302668</td>\n",
       "      <td>0.758852</td>\n",
       "      <td>0.599164</td>\n",
       "      <td>0.628630</td>\n",
       "      <td>0.621854</td>\n",
       "      <td>0.583705</td>\n",
       "      <td>0.628376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740585</td>\n",
       "      <td>0.741476</td>\n",
       "      <td>0.689943</td>\n",
       "      <td>0.696236</td>\n",
       "      <td>0.591465</td>\n",
       "      <td>0.554220</td>\n",
       "      <td>0.731547</td>\n",
       "      <td>0.740719</td>\n",
       "      <td>0.521373</td>\n",
       "      <td>0.447511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>0.074528</td>\n",
       "      <td>0.437942</td>\n",
       "      <td>0.106036</td>\n",
       "      <td>0.305981</td>\n",
       "      <td>0.752417</td>\n",
       "      <td>0.605011</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.616156</td>\n",
       "      <td>0.580098</td>\n",
       "      <td>0.620269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752734</td>\n",
       "      <td>0.758983</td>\n",
       "      <td>0.697934</td>\n",
       "      <td>0.705280</td>\n",
       "      <td>0.589566</td>\n",
       "      <td>0.560898</td>\n",
       "      <td>0.730307</td>\n",
       "      <td>0.742093</td>\n",
       "      <td>0.524449</td>\n",
       "      <td>0.443889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1n        2n        3n        4n        5n        6r        7r  \\\n",
       "0    0.022902  0.662567  0.034351  0.148139  0.493378  0.671918  0.615551   \n",
       "1    0.024732  0.658107  0.033129  0.146617  0.478424  0.667990  0.610094   \n",
       "2    0.026979  0.653591  0.031421  0.148883  0.486036  0.669903  0.621272   \n",
       "3    0.030909  0.649135  0.030034  0.146710  0.451634  0.666542  0.638415   \n",
       "4    0.030741  0.640175  0.027405  0.147321  0.464734  0.664265  0.632699   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "520  0.081394  0.446675  0.091244  0.298324  0.763869  0.582584  0.637223   \n",
       "521  0.089386  0.458004  0.096382  0.299258  0.765427  0.577321  0.635280   \n",
       "522  0.078236  0.429350  0.099725  0.304498  0.745902  0.593526  0.630973   \n",
       "523  0.071266  0.421421  0.102860  0.302668  0.758852  0.599164  0.628630   \n",
       "524  0.074528  0.437942  0.106036  0.305981  0.752417  0.605011  0.633200   \n",
       "\n",
       "           8r        9r       10r  ...       92r       93r       94r  \\\n",
       "0    0.625976  0.647064  0.578182  ...  0.619049  0.748539  0.588579   \n",
       "1    0.620036  0.641139  0.574126  ...  0.603915  0.752731  0.587314   \n",
       "2    0.623157  0.634809  0.576202  ...  0.616818  0.754496  0.590404   \n",
       "3    0.608425  0.667469  0.572128  ...  0.600729  0.732823  0.590608   \n",
       "4    0.612745  0.652520  0.567949  ...  0.583046  0.741828  0.588947   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "520  0.610751  0.585631  0.609958  ...  0.697665  0.694346  0.705105   \n",
       "521  0.600362  0.587635  0.615288  ...  0.715464  0.712684  0.704778   \n",
       "522  0.628439  0.581926  0.623445  ...  0.748459  0.751056  0.692863   \n",
       "523  0.621854  0.583705  0.628376  ...  0.740585  0.741476  0.689943   \n",
       "524  0.616156  0.580098  0.620269  ...  0.752734  0.758983  0.697934   \n",
       "\n",
       "          95r       96r       97r       98r       99r      100r      101r  \n",
       "0    0.679175  0.744493  0.587385  0.571649  0.732362  0.743735  0.530578  \n",
       "1    0.685208  0.741377  0.585352  0.564472  0.732922  0.743326  0.532095  \n",
       "2    0.676380  0.738477  0.582982  0.568050  0.733060  0.743631  0.527555  \n",
       "3    0.664383  0.735578  0.575488  0.575268  0.732713  0.742071  0.541149  \n",
       "4    0.667850  0.730141  0.577973  0.579028  0.729647  0.740905  0.534735  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "520  0.740672  0.596873  0.551078  0.727091  0.739228  0.512227  0.458557  \n",
       "521  0.732992  0.595095  0.545270  0.725116  0.736531  0.509141  0.455400  \n",
       "522  0.731864  0.593254  0.557480  0.728852  0.742128  0.518212  0.439950  \n",
       "523  0.696236  0.591465  0.554220  0.731547  0.740719  0.521373  0.447511  \n",
       "524  0.705280  0.589566  0.560898  0.730307  0.742093  0.524449  0.443889  \n",
       "\n",
       "[525 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[]\n",
    "clean_data=[]\n",
    "for i in data:\n",
    "    label.append(i[-1])  #get all the labels from the column name\n",
    "    clean_data.append(data[i].to_numpy())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r',\n",
       " 'r']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y=np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n', 'n', 'n', 'n', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'n', 'n', 'n', 'n', 'n', 'n', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'], dtype='<U1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=np.array(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=np.expand_dims(np.random.normal(size=data_x.shape[0:2]),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 525)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y.shape\n",
    "integer_encoded\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=80, kernel_size=100, activation='relu', input_shape=data_x.shape[1:]))\n",
    "#model.add(Conv1D(filters=20, kernel_size=300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 31 samples\n",
      "Epoch 1/6\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 2.5542 - accuracy: 0.4357 - val_loss: 0.6205 - val_accuracy: 0.5484\n",
      "Epoch 2/6\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.8232 - accuracy: 0.6357 - val_loss: 0.5112 - val_accuracy: 0.8065\n",
      "Epoch 3/6\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.7153 - accuracy: 0.5857 - val_loss: 0.5237 - val_accuracy: 0.8065\n",
      "Epoch 4/6\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.6357 - accuracy: 0.6143 - val_loss: 0.6052 - val_accuracy: 0.8065\n",
      "Epoch 5/6\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.5236 - accuracy: 0.8143 - val_loss: 0.6294 - val_accuracy: 0.7903\n",
      "Epoch 6/6\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4243 - accuracy: 0.9000 - val_loss: 0.5616 - val_accuracy: 0.8065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_x, onehot_encoded, epochs=6, batch_size=16,validation_split=0.3,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf11",
   "language": "python",
   "name": "tf11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
